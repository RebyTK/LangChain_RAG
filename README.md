# RAG Question-Answering System

Retrieval-Augmented Generation (RAG) demo built with LangChain. The system ingests a small corpus of local documents, indexes them with sentence-transformer embeddings, and answers user questions with grounded responses generated by an Ollama-served Llama 2 model. Both a CLI and a Streamlit UI are provided.

## Features
- LangChain-based pipeline with document loading, chunking, vectorization (Chroma), and retrieval QA.
- Local LLM via Ollama (default `llama2`, configurable model/base URL).
- Source citations returned for every answer.
- Streamlit web experience with configurable parameters and chat history.
- CLI for scripted or interactive sessions.
- Sample documents + `setup.sh` bootstrap script to create env, pull dependencies, and run tests.

## Architecture
```
┌──────────────┐   load/chunk   ┌──────────────────┐   embeddings   ┌──────────────┐
│documents (.txt│ ─────────────▶ │DocumentProcessor │ ─────────────▶ │Chroma Vector │
│/.md in data/) │                └──────────────────┘                │Store         │
└──────────────┘                                  ▲                 └──────┬───────┘
                                                  │ retrieve (k)          │
                                      ┌───────────┴──────────┐            │
                                      │ RetrievalQA Chain    │◀───────────┘
                                      │ (LangChain prompt +  │
                                      │ Ollama Llama 2 LLM)  │
                                      └───────────┬──────────┘
                                                  │
                                     answer + citations
                                                  │
                                  CLI (`cli.py`) / Streamlit (`app.py`)
```

### Key modules
- `main.py`
  - `RAGConfig`: centralizes all runtime settings (paths, chunk sizes, LLM params, Ollama URL).
  - `DocumentProcessor`: loads `.txt`/`.md` files and splits them with `RecursiveCharacterTextSplitter`.
  - `VectorStoreManager`: builds/persists `Chroma` DB using HuggingFace embeddings (`all-MiniLM-L6-v2`).
  - `RAGPipeline`: orchestrates setup, retrieval QA chain, and question handling.
- `cli.py`: argparse-driven interface for single-shot or interactive Q&A, reindexing, stats, etc.
- `app.py`: Streamlit front-end with configuration sidebar, chat log, citations, and metrics.
- `test_rag.py`: unittest suite covering document loading/chunking, configuration, and basic retrieval.
- `setup.sh`: optional helper for Unix-like shells to create venv, install deps, seed sample docs, and run tests.

## Prerequisites
- Python 3.9+
- [Ollama](https://ollama.com/download) installed locally
  - After installation run: `ollama pull llama2`
  - Start the service before querying: `ollama serve` (runs at `http://localhost:11434` by default)

## Quick Start
```bash
# Clone the repository
git clone <repo-url>
cd LangChain_RAG

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# or source venv/bin/activate on macOS/Linux

# Install dependencies
cd rag_qa_system
pip install -r requirements.txt
```

### (Optional) Automated bootstrap
```bash
chmod +x setup.sh
./setup.sh
```
The script verifies Python, creates the venv, installs dependencies, seeds `data/documents/` with sample files, and runs unit tests. It also reminds you to install/run Ollama.

## Adding Documents
Place 10–20 public `.txt` or `.md` files inside `data/documents/`. You can reuse the seeded examples or delete them and substitute your own knowledge base. The `setup()` flow will reindex automatically the first time; use `force_reindex=True`, the CLI `reindex` command, or the Streamlit “Reindex” button after changing content.

## Running the CLI
```bash
cd rag_qa_system
python cli.py                              # interactive shell
python cli.py "What is Python?"            # single question
python cli.py --reindex                    # rebuild vector store
python cli.py --llm-model llama3 --top-k 5 # override defaults
```
Environment overrides:
- `OLLAMA_MODEL` – default LLM (`llama2`)
- `OLLAMA_BASE_URL` – default `http://localhost:11434`

## Running the Streamlit App
```bash
cd rag_qa_system
streamlit run app.py
```
Use the sidebar to adjust chunk size, overlap, Top-K, temperature, model name, and Ollama URL. Chat history and citations appear in the main pane.

## Tests
```bash
# From project root
python tests/test_rag.py
```
The suite relies on temporary directories and small synthetic docs; no external services are required beyond embeddings/Chroma.

## Troubleshooting
- **Ollama not running**: Start `ollama serve` and ensure the chosen model is pulled (`ollama pull llama2`).
- **Embedding download slow**: HuggingFace models download on first run; prefetch by running `python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"` inside the venv.
- **Vector store mismatch**: Delete `data/vector_store/` or use the `reindex` flow after changing docs or embeddings.

## Extending
- Swap embeddings or vector stores by modifying `RAGConfig`.
- Plug in different Ollama models (e.g., `llama3`, `mistral`) via CLI/UI/env overrides.
- Add conversation memory or follow-up question logic by wrapping `RAGPipeline.query`.
- Integrate authentication or advanced logging before deploying.

Enjoy experimenting with the RAG stack! Feel free to adapt the structure to larger datasets or alternative LLM backends.

