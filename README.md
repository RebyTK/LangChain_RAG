# RAG Question-Answering System

Retrieval-Augmented Generation (RAG) demo built with LangChain. The system ingests a small corpus of local documents, indexes them with sentence-transformer embeddings, and answers user questions with grounded responses generated by an Ollama-served Llama 2 model. Both a CLI and a Streamlit UI are provided.

## Features
- LangChain-based pipeline with document loading, chunking, vectorization (Chroma), and retrieval QA.
- Local LLM via Ollama (default `llama2`, configurable model/base URL).
- Source citations returned for every answer.
- Streamlit web experience with configurable parameters and chat history.
- CLI for scripted or interactive sessions.
- Sample documents + `setup.sh` bootstrap script to create env, pull dependencies, and run tests.

## Architecture
```
┌──────────────┐   load/chunk   ┌──────────────────┐   embeddings   ┌──────────────┐
│documents (.txt│ ─────────────▶ │DocumentProcessor │ ─────────────▶ │Chroma Vector │
│/.md in data/) │                └──────────────────┘                │Store         │
└──────────────┘                                  ▲                 └──────┬───────┘
                                                  │ retrieve (k)          │
                                      ┌───────────┴──────────┐            │
                                      │ RetrievalQA Chain    │◀───────────┘
                                      │ (LangChain prompt +  │
                                      │ Ollama Llama 2 LLM)  │
                                      └───────────┬──────────┘
                                                  │
                                     answer + citations
                                                  │
                                  CLI (`cli.py`) / Streamlit (`app.py`)
```

### Key modules
- `main.py`
  - `RAGConfig`: centralizes all runtime settings (paths, chunk sizes, LLM params, Ollama URL).
  - `DocumentProcessor`: loads `.txt`/`.md` files and splits them with `RecursiveCharacterTextSplitter`.
  - `VectorStoreManager`: builds/persists `Chroma` DB using HuggingFace embeddings (`all-MiniLM-L6-v2`).
  - `RAGPipeline`: orchestrates setup, retrieval QA chain, and question handling.
- `cli.py`: argparse-driven interface for single-shot or interactive Q&A, reindexing, stats, etc.
- `app.py`: Streamlit front-end with configuration sidebar, chat log, citations, and metrics.
- `test_rag.py`: unittest suite covering document loading/chunking, configuration, and basic retrieval.
- `setup.sh`: optional helper for Unix-like shells to create venv, install deps, seed sample docs, and run tests.

## Prerequisites
- Python 3.9+
- Ollama installed locally (see installation instructions below)

## Installing Ollama and Llama2

### Step 1: Install Ollama

Download and install Ollama from [https://ollama.com/download](https://ollama.com/download)

**Windows:**
1. Download the installer from the Ollama website
2. Run the installer (`ollama-windows-amd64.exe`)
3. Follow the installation wizard
4. Ollama will start automatically after installation


### Step 2: Download Llama2 Model

After Ollama is installed, download the Llama2 model:

```bash
ollama pull llama2
```

This will download the Llama2 model (approximately 3.8GB). The download may take a few minutes depending on your internet connection.

**Verify installation:**
```bash
ollama list
```

You should see `llama2` in the list of available models.

### Step 3: Start Ollama Service

Ollama runs as a background service. Make sure it's running before using the RAG system:

**Windows:**
- Ollama should start automatically after installation
- If not, search for "Ollama" in the Start menu and launch it

**macOS/Linux:**
```bash
ollama serve
```

The service will run at `http://localhost:11434` by default.


## Quick Start

### Step 1: Ensure Ollama is Running

Before starting, make sure Ollama is installed and running (see [Installing Ollama and Llama2](#installing-ollama-and-llama2) section above).

```bash
# Verify Ollama is running
curl http://localhost:11434/api/tags  # macOS/Linux
# or open http://localhost:11434/api/tags in your browser (Windows)
```

### Step 2: Set Up Python Environment

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
venv\Scripts\activate  # Windows
# or source venv/bin/activate on macOS/Linux

# Install dependencies
cd rag_qa_system
pip install -r requirements.txt
```

## Adding Documents
Place 10–20 public `.txt` or `.md` files inside `data/documents/`. You can reuse the seeded examples or delete them and substitute your own knowledge base. The `setup()` flow will reindex automatically the first time; use `force_reindex=True`, the CLI `reindex` command, or the Streamlit “Reindex” button after changing content.

## Running the CLI
```bash
cd rag_qa_system
python cli.py                              # interactive shell
python cli.py "What is Python?"            # single question
python cli.py --reindex                    # rebuild vector store
python cli.py --llm-model llama3 --top-k 5 # override defaults
```
Environment overrides:
- `OLLAMA_MODEL` – default LLM (`llama2`)
- `OLLAMA_BASE_URL` – default `http://localhost:11434`

## Running the Streamlit App
```bash
cd rag_qa_system
streamlit run app.py
```
Use the sidebar to adjust chunk size, overlap, Top-K, temperature, model name, and Ollama URL. Chat history and citations appear in the main pane.

## Tests
```bash
# From project root
python tests/test_rag.py
```
The suite relies on temporary directories and small synthetic docs; no external services are required beyond embeddings/Chroma.

## Troubleshooting
- **Ollama not running**: Start `ollama serve` and ensure the chosen model is pulled (`ollama pull llama2`).
- **Embedding download slow**: HuggingFace models download on first run; prefetch by running `python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"` inside the venv.
- **Vector store mismatch**: Delete `data/vector_store/` or use the `reindex` flow after changing docs or embeddings.

## Extending
- Swap embeddings or vector stores by modifying `RAGConfig`.
- Plug in different Ollama models (e.g., `llama3`, `mistral`) via CLI/UI/env overrides.
- Add conversation memory or follow-up question logic by wrapping `RAGPipeline.query`.
- Integrate authentication or advanced logging before deploying.

Enjoy experimenting with the RAG stack! Feel free to adapt the structure to larger datasets or alternative LLM backends.

